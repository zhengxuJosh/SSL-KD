# Overview
Knowledge distillation is the process of transfering from a large model to a smaller one. While large models have knowledge capacity than small models, this capacity might not be utilized.

|Year|Paper|HighLight|
|:----:|:----:|:----:|
|2015|[Distilling the Knowledge in a Neural Network](https://users.soe.ucsc.edu/~pang/200/f18/papers/2018/1503.02531.pdf)|Temperature & Dark Knowledge|
|2015|Fitnets: Hints for thin deep nets|Hint-based Training|
|2017|Paying more Attention to Attention|Similar Spatial Attention Maps|
|2017|[Learning Efficient Object Detection Models](https://proceedings.neurips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf)|Hint Learning & Weighted CE|
|2017|[A gift from knowledge distillation: Fast optimization, network minimization and transfer learnin](https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf)|FSP Matrix|
|2018|DarkRank:Accelerating Deep Metric Learning via Cross Sample Similarities Transfer|Cross Sample Similarities Transfer|
|2018|[Born-Again Neural Networks](https://arxiv.org/pdf/1805.04770.pdf)|**BANs**|
|2019|Contrastive Representation Distillation|Contrastive Learning|
|2019|Teacher Assistant Knowledge Distillation|Multi-step distillation|
|2019|[On the Efficacy of Knowledge Distillation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Cho_On_the_Efficacy_of_Knowledge_Distillation_ICCV_2019_paper.pdf)|Mismatched Capacity|
|2019|[A Comprehensive Overhaul of Feature Distillation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Heo_A_Comprehensive_Overhaul_of_Feature_Distillation_ICCV_2019_paper.pdf)|MAgnitude & Activation Status|
|2019|Route Constrained Optimization|Route Contrained Hint Learning(Easy to Hard)|
|2019|[Search to Distill: Pearls Are Everywhere but Not the Eyes](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Search_to_Distill_Pearls_Are_Everywhere_but_Not_the_Eyes_CVPR_2020_paper.pdf)|Architecture-aware Knowledge Distillation (AKD)|
|2019|[Towards Understanding Knowledge Distillation](http://proceedings.mlr.press/v97/phuong19a/phuong19a.pdf)||
|2019|[Structured Knowledge Distillation for Semantic Segmentation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.pdf)|Pixel-wise Distillation & Structured Knowledge|

