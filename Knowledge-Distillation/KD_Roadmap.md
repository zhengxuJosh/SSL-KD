# Overview
Knowledge distillation is the process of transfering from a large model to a smaller one. While large models have knowledge capacity than small models, this capacity might not be utilized.

|Year|Paper|HighLight|
|:----:|:----:|:----:|
|2015|[Distilling the Knowledge in a Neural Network](https://users.soe.ucsc.edu/~pang/200/f18/papers/2018/1503.02531.pdf)|Temperature & Dark Knowledge|
|2015|[Fitnets: Hints for thin deep nets]()|Hint-based Training|
