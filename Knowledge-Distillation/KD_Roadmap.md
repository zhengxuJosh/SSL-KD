# Overview
Knowledge distillation is the process of transfering from a large model to a smaller one. While large models have knowledge capacity than small models, this capacity might not be utilized.
## Paper List
|Year|Paper|HighLight|
|:----:|:----:|:----:|
|2015|[Distilling the Knowledge in a Neural Network](https://users.soe.ucsc.edu/~pang/200/f18/papers/2018/1503.02531.pdf)|Temperature & Dark Knowledge|
|2015|Fitnets: Hints for thin deep nets|Hint-based Training|
|2017|[Paying more Attention to Attention](https://arxiv.org/pdf/1612.03928.pdf)|Similar Spatial Attention Maps|
|2017|[Learning Efficient Object Detection Models](https://proceedings.neurips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf)|Hint Learning & Weighted CE|
|2017|[A gift from knowledge distillation: Fast optimization, network minimization and transfer learnin](https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf)|FSP Matrix|
|2018|DarkRank:Accelerating Deep Metric Learning via Cross Sample Similarities Transfer|Cross Sample Similarities Transfer|
|2018|[Born-Again Neural Networks](https://arxiv.org/pdf/1805.04770.pdf)|**BANs**|
|2019|Contrastive Representation Distillation|Contrastive Learning|
|2019|Teacher Assistant Knowledge Distillation|Multi-step distillation|
|2019|[On the Efficacy of Knowledge Distillation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Cho_On_the_Efficacy_of_Knowledge_Distillation_ICCV_2019_paper.pdf)|Mismatched Capacity|
|2019|[A Comprehensive Overhaul of Feature Distillation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Heo_A_Comprehensive_Overhaul_of_Feature_Distillation_ICCV_2019_paper.pdf)|MAgnitude & Activation Status|
|2019|Route Constrained Optimization|Route Contrained Hint Learning(Easy to Hard)|
|2019|[Search to Distill: Pearls Are Everywhere but Not the Eyes](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Search_to_Distill_Pearls_Are_Everywhere_but_Not_the_Eyes_CVPR_2020_paper.pdf)|Architecture-aware Knowledge Distillation (AKD)|
|2019|[Towards Understanding Knowledge Distillation](http://proceedings.mlr.press/v97/phuong19a/phuong19a.pdf)||
|2019|[Structured Knowledge Distillation for Semantic Segmentation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.pdf)|Pixel-wise Distillation & Structured Knowledge|
|2022|[Extractive Knowledge Distillation](https://openaccess.thecvf.com/content/WACV2022/papers/Kobayashi_Extractive_Knowledge_Distillation_WACV_2022_paper.pdf)||
|2022|[Distilling a Powerful Student Model via Online Knowledge Distillaiton](https://arxiv.org/pdf/2103.14473.pdf)||
|2022|[Dynamic Rectification Knowledge Distillation](https://arxiv.org/pdf/2201.11319.pdf)||
#### 2015 Distilling the Knowledge in a Neural Network
Significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model.

#### 2015 Fitnets: Hints for thin deep nets
Using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student.

#### 2017 Pay more Attention to Attention
By properly defining attention for CNN, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network.

#### 2017 Learning Efficient Object Detection Models with Knowledge Distillation
Combining the knowledge distillation and hint framework together.

#### 2017 A gift from knowledge distillation: Fast optimization, network minimization and transfer learnin
Transfer the distilled knoledge by FSP matrix(generated by the features from two layers).

#### 2017 DarkRank:Accelerating Deep Metric Learning via Cross Sample Similarities Transfer
