## Overview
Knowledge distillation is the process of transfering from a large model to a smaller one. While large models have knowledge capacity than small models, this capacity might not be utilized.
