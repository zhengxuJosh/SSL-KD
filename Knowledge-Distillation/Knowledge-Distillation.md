## Paper Summary
| Domian | Conference | Title | Code |
| :-------------: | :-------------: | :-------------: | :-------------: |
|KD| CVPR 2019 | [Relational Knowledge Distillation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.pdf)||
|KD| ICCV 2019 | [Similarity-Preserving Knowledge Distillation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.pdf)||
|KD| CVPR 2020 | [Revisiting Knowledge Distillation via Label Smoothing Regularization](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yuan_Revisiting_Knowledge_Distillation_via_Label_Smoothing_Regularization_CVPR_2020_paper.pdf)||
|KD| CVPR 2022 | [Wavelet Knowledge Distillation: Towards Efficient Image-to-Image Translation](https://arxiv.org/pdf/2203.06321)||
|KD| CVPR 2022 | [ISD: Self-Supervised Learning by Iterative Similarity Distillation](https://openaccess.thecvf.com/content/ICCV2021/papers/Tejankar_ISD_Self-Supervised_Learning_by_Iterative_Similarity_Distillation_ICCV_2021_paper.pdf)||
|KD| CVPR 2022 | [Decoupled Knowledge Distillation](https://arxiv.org/pdf/2203.08679) ||
|KD| CVPR 2022 | [Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability](https://arxiv.org/pdf/2203.05180) ||
|KD| CVPR 2022 | [Focal and Global Knowledge Distillation for Detectors](https://arxiv.org/pdf/2111.11837) ||
|KD| ICLR 2022 | [Unsupervised Semantic Segmentation by Distilling Feature Correspondences](https://openreview.net/pdf?id=SaKO6z6Hl0c) |
|KD| ICLR 2022 | [Reliable Adversarial Distillation with Unreliable Teachers](https://openreview.net/pdf?id=u6TRGdzhfip) |

