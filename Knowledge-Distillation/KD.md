## Method Comparison
| Domian | Conference | Title | Code |
| :-------------: | :-------------: | :-------------: | :-------------: | 

## Paper Summary
| Domian | Conference | Title | Code |
| :-------------: | :-------------: | :-------------: | :-------------: | 
|KD| CVPR 2022 | [Wavelet Knowledge Distillation: Towards Efficient Image-to-Image Translation](https://arxiv.org/pdf/2203.06321)||
|KD| CVPR 2022 | [ISD: Self-Supervised Learning by Iterative Similarity Distillation](https://openaccess.thecvf.com/content/ICCV2021/papers/Tejankar_ISD_Self-Supervised_Learning_by_Iterative_Similarity_Distillation_ICCV_2021_paper.pdf)||
|KD| CVPR 2022 | [Decoupled Knowledge Distillation](https://arxiv.org/pdf/2203.08679) ||
|KD| CVPR 2022 | [Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability](https://arxiv.org/pdf/2203.05180) ||
|KD| CVPR 2022 | [Focal and Global Knowledge Distillation for Detectors](https://arxiv.org/pdf/2111.11837) ||
|KD| ICLR 2022 | [Unsupervised Semantic Segmentation by Distilling Feature Correspondences](https://openreview.net/pdf?id=SaKO6z6Hl0c) |
|KD| ICLR 2022 | [Reliable Adversarial Distillation with Unreliable Teachers](https://openreview.net/pdf?id=u6TRGdzhfip) |

